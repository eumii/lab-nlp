{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: Sentiment Analysis\n",
    "\n",
    "In this challenge we will learn sentiment analysis and practice performing sentiment analysis on Twitter tweets. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sentiment analysis is to *systematically identify, extract, quantify, and study affective states and subjective information* based on texts ([reference](https://en.wikipedia.org/wiki/Sentiment_analysis)). In simple words, it's to understand whether a person is happy or unhappy in producing the piece of text. Why we (or rather, companies) care about sentiment in texts? It's because by understanding the sentiments in texts, we will be able to know if our customers are happy or unhappy about our products and services. If they are unhappy, the subsequent action is to figure out what have caused the unhappiness and make improvements.\n",
    "\n",
    "Basic sentiment analysis only understands the *positive* or *negative* (sometimes *neutral* too) polarities of the sentiment. More advanced sentiment analysis will also consider dimensions such as agreement, subjectivity, confidence, irony, and so on. In this challenge we will conduct the basic positive vs negative sentiment analysis based on real Twitter tweets.\n",
    "\n",
    "NLTK comes with a [sentiment analysis package](https://www.nltk.org/api/nltk.sentiment.html). This package is great for dummies to perform sentiment analysis because it requires only the textual data to make predictions. For example:\n",
    "\n",
    "```python\n",
    ">>> from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    ">>> txt = \"Ironhack is a Global Tech School ranked num 2 worldwide.  \n",
    " \n",
    "Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.\"\n",
    ">>> analyzer = SentimentIntensityAnalyzer()\n",
    ">>> analyzer.polarity_scores(txt)\n",
    "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.8442}\n",
    "```\n",
    "\n",
    "In this challenge, however, you will not use NLTK's sentiment analysis package because in your Machine Learning training in the past 2 weeks you have learned how to make predictions more accurate than that. The [tweets data](https://www.kaggle.com/kazanova/sentiment140) we will be using today are already coded for the positive/negative sentiment. You will be able to use the Naïve Bayes classifier you learned in the lesson to predict the sentiment of tweets based on the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting Sentiment Analysis\n",
    "\n",
    "### Loading and Exploring Data\n",
    "\n",
    "The dataset we'll be using today is located on Kaggle (https://www.kaggle.com/kazanova/sentiment140). Once you have downloaded and imported the dataset, it you will need to define the columns names: df.columns = ['target','id','date','flag','user','text']\n",
    "\n",
    "*Notes:* \n",
    "\n",
    "* The dataset is huuuuge (1.6m tweets). When you develop your data analysis codes, you can sample a subset of the data (e.g. 20k records) so that you will save a lot of time when you test your codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kaggle.com/datasets/kazanova/sentiment140'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<title>Sentiment140 dataset with 1.6 million tweets | Kaggle</title>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"index, follow\" name=\"robots\"/>\n",
       "<meta content=\"Sentiment analysis with tweets\" name=\"description\"/>\n",
       "<meta content=\"no-cache\" name=\"turbolinks-cache-control\"/>\n",
       "<meta content=\"internet,online communities,social networks,linguistics,languages\" name=\"keywords\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0\" name=\"viewport\"/>\n",
       "<meta content=\"#008ABC\" name=\"theme-color\">\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\" type=\"text/javascript\">\n",
       "    window[\"pageRequestStartTime\"] = 1675330178854;\n",
       "    window[\"pageRequestEndTime\"] = 1675330178966;\n",
       "    window[\"initialPageLoadStartTime\"] = new Date().getTime();\n",
       "  </script>\n",
       "<link crossorigin=\"anonymous\" href=\"https://www.google-analytics.com\" rel=\"preconnect\"/><link href=\"https://stats.g.doubleclick.net\" rel=\"preconnect\"/><link href=\"https://storage.googleapis.com\" rel=\"preconnect\"/><link href=\"https://apis.google.com\" rel=\"preconnect\"/>\n",
       "<link href=\"/static/images/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/>\n",
       "<link crossorigin=\"use-credentials\" href=\"/static/json/manifest.json\" rel=\"manifest\"/>\n",
       "<link crossorigin=\"\" href=\"https://fonts.gstatic.com\" rel=\"preconnect\">\n",
       "<link as=\"style\" href=\"https://fonts.googleapis.com/icon?family=Google+Material+Icons\" rel=\"preload\"/>\n",
       "<link as=\"style\" href=\"https://fonts.googleapis.com/css?family=Inter:400,400i,500,500i,600,600i,700,700i\" rel=\"preload\"/>\n",
       "<link href=\"https://fonts.googleapis.com/icon?family=Google+Material+Icons\" id=\"async-google-font-1\" media=\"print\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Inter:400,400i,500,500i,600,600i,700,700i\" id=\"async-google-font-2\" media=\"print\" rel=\"stylesheet\"/>\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\" type=\"text/javascript\">\n",
       "    const styleSheetIds = [\"async-google-font-1\", \"async-google-font-2\"];\n",
       "    styleSheetIds.forEach(function (id) {\n",
       "      document.getElementById(id).addEventListener(\"load\", function() {\n",
       "        this.media = \"all\";\n",
       "      });\n",
       "    });\n",
       "  </script>\n",
       "<link href=\"/datasets/kazanova/sentiment140\" rel=\"canonical\"/>\n",
       "<link href=\"/static/assets/vendor.css?v=a74252eff52838d277da\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<link href=\"/static/assets/app.css?v=8cee4fa34d4da0b85dfc\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\">\n",
       "        try{(function(a,s,y,n,c,h,i,d,e){d=s.createElement(\"style\");\n",
       "        d.appendChild(s.createTextNode(\"\"));s.head.appendChild(d);d=d.sheet;\n",
       "        y=y.map(x => d.insertRule(x + \"{ opacity: 0 !important }\"));\n",
       "        h.start=1*new Date;h.end=i=function(){y.forEach(x => x<d.cssRules.length ? d.deleteRule(x) : {})};\n",
       "        (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;\n",
       "        })(window,document,['.site-header-react__nav'],'dataLayer',2000,{'GTM-52LNT9S':true});}catch(ex){}\n",
       "    </script>\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\">\n",
       "        window.dataLayer = window.dataLayer || [];\n",
       "        function gtag() { dataLayer.push(arguments); }\n",
       "        gtag('js', new Date());\n",
       "        gtag('config', 'UA-12629138-1', {\n",
       "            'optimize_id': 'GTM-52LNT9S',\n",
       "            'displayFeaturesTask': null,\n",
       "            'send_page_view': false,\n",
       "            'content_group1': 'Datasets'\n",
       "        });\n",
       "    </script>\n",
       "<script async=\"\" nonce=\"GpPhwX4/E337PCNOSKRthg==\" src=\"https://www.googletagmanager.com/gtag/js?id=UA-12629138-1\"></script>\n",
       "<meta content=\"/datasets/kazanova/sentiment140\" property=\"og:url\"/>\n",
       "<meta content=\"Sentiment140 dataset with 1.6 million tweets\" property=\"og:title\"/>\n",
       "<meta content=\"Sentiment analysis with tweets\" property=\"og:description\"/>\n",
       "<meta content=\"website\" property=\"og:type\"/>\n",
       "<meta content=\"https://storage.googleapis.com/kaggle-datasets-images/2477/4136/16575d5fdeeba8a6b94021e4bf2748d0/dataset-card.jpg\" property=\"og:image\"/>\n",
       "<meta content=\"2665027677054710\" property=\"fb:app_id\"/>\n",
       "<meta content=\"summary\" name=\"twitter:card\"/>\n",
       "<meta content=\"@kaggledatasets\" name=\"twitter:site\"/>\n",
       "<meta content=\"@Kaggle\" name=\"twitter:site\"/>\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\" type=\"application/ld+json\">{\"@context\":\"http://schema.org/\",\"@type\":\"Dataset\",\"name\":\"Sentiment140 dataset with 1.6 million tweets\",\"description\":\"### Context\\n\\nThis is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\\n\\n### Content\\n\\nIt contains the following 6 fields:\\n\\n1. **target**: the polarity of the tweet (*0* = negative, *2* = neutral, *4* = positive)\\n\\n2. **ids**: The id of the tweet ( *2087*)\\n\\n3. **date**: the date of the tweet (*Sat May 16 23:58:44 UTC 2009*)\\n\\n4. **flag**: The query (*lyx*). If there is no query, then this value is NO_QUERY.\\n\\n5. **user**: the user that tweeted (*robotickilldozr*)\\n\\n6.  **text**: the text of the tweet (*Lyx is cool*)\\n\\n\\n### Acknowledgements\\n\\nThe official link regarding the dataset with resources about how it was generated is [here][1]\\nThe official paper detailing the approach is  [here][2]\\n\\nCitation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. *CS224N Project Report, Stanford, 1(2009), p.12*.\\n\\n\\n### Inspiration\\n\\nTo detect severity from tweets. You [may have a look at this][3].\\n\\n[1]: http://%20http://help.sentiment140.com/for-students/\\n[2]: http://bhttp://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\\n[3]: https://www.linkedin.com/pulse/social-machine-learning-h2o-twitter-python-marios-michailidis\",\"url\":\"https://www.kaggle.com/kazanova/sentiment140\",\"version\":2,\"keywords\":[\"subject, science and technology, internet\",\"subject, science and technology, internet, online communities\",\"subject, science and technology, internet, online communities, social networks\",\"subject, people and society, social science, linguistics\",\"subject, culture and humanities, languages\"],\"license\":{\"@type\":\"CreativeWork\",\"name\":\"Other (specified in description)\",\"url\":\"\"},\"identifier\":[\"2477\"],\"includedInDataCatalog\":{\"@type\":\"DataCatalog\",\"name\":\"Kaggle\",\"url\":\"https://www.kaggle.com\"},\"creator\":{\"@type\":\"Person\",\"name\":\"Μαριος Μιχαηλιδης KazAnova\",\"url\":\"https://www.kaggle.com/kazanova\",\"image\":\"https://storage.googleapis.com/kaggle-avatars/thumbnails/111640-fb.jpg\"},\"distribution\":[{\"@type\":\"DataDownload\",\"requiresSubscription\":true,\"encodingFormat\":\"zip\",\"fileFormat\":\"zip\",\"contentUrl\":\"https://www.kaggle.com/datasets/kazanova/sentiment140/download?datasetVersionNumber=2\",\"contentSize\":\"84855679 bytes\"}],\"commentCount\":19,\"dateModified\":\"2017-09-13T22:43:19.117Z\",\"discussionUrl\":\"https://www.kaggle.com/kazanova/sentiment140/discussion\",\"alternateName\":\"Sentiment analysis with tweets\",\"isAccessibleForFree\":true,\"thumbnailUrl\":\"https://storage.googleapis.com/kaggle-datasets-images/2477/4136/16575d5fdeeba8a6b94021e4bf2748d0/dataset-card.jpg\",\"interactionStatistic\":[{\"@type\":\"InteractionCounter\",\"interactionType\":\"http://schema.org/CommentAction\",\"userInteractionCount\":19},{\"@type\":\"InteractionCounter\",\"interactionType\":\"http://schema.org/DownloadAction\",\"userInteractionCount\":97607},{\"@type\":\"InteractionCounter\",\"interactionType\":\"http://schema.org/ViewAction\",\"userInteractionCount\":639681},{\"@type\":\"InteractionCounter\",\"interactionType\":\"http://schema.org/LikeAction\",\"userInteractionCount\":1528}]}</script>\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\">window['useKaggleAnalytics'] = true;</script>\n",
       "<script async=\"\" defer=\"\" id=\"gapi-target\" nonce=\"GpPhwX4/E337PCNOSKRthg==\" src=\"https://apis.google.com/js/api.js\"></script>\n",
       "<script data-turbolinks-track=\"reload\" nonce=\"GpPhwX4/E337PCNOSKRthg==\" src=\"/static/assets/runtime.js?v=cefc238cb3e5e2cb953c\"></script>\n",
       "<script data-turbolinks-track=\"reload\" nonce=\"GpPhwX4/E337PCNOSKRthg==\" src=\"/static/assets/vendor.js?v=02cf5abda305fe8f0ba1\"></script>\n",
       "<script data-turbolinks-track=\"reload\" nonce=\"GpPhwX4/E337PCNOSKRthg==\" src=\"/static/assets/app.js?v=a5ef665d0f1546aad17d\"></script>\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\" type=\"text/javascript\">\n",
       "      window.kaggleStackdriverConfig = {\n",
       "        key: 'AIzaSyA4eNqUdRRskJsCZWVz-qL655Xa5JEMreE',\n",
       "        projectId: 'kaggle-161607',\n",
       "        service: 'web-fe',\n",
       "        version: 'ci',\n",
       "        userId: '0'\n",
       "      }\n",
       "    </script>\n",
       "</link></meta></head>\n",
       "<body data-turbolinks=\"false\">\n",
       "<main>\n",
       "<div id=\"site-container\"></div>\n",
       "<div class=\"hide\" id=\"site-body\">\n",
       "<div <=\"\" data-component-name=\"DatasetMaterialContainer\" div=\"\" style=\"display: flex; flex-direction: column; flex: 1 0 auto;\"><script class=\"kaggle-component\" nonce=\"GpPhwX4/E337PCNOSKRthg==\">var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({\"basics\":{\"datasetId\":2477,\"slug\":\"sentiment140\",\"title\":\"Sentiment140 dataset with 1.6 million tweets\",\"description\":\"### Context\\n\\nThis is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\\n\\n### Content\\n\\nIt contains the following 6 fields:\\n\\n1. **target**: the polarity of the tweet (*0* = negative, *2* = neutral, *4* = positive)\\n\\n2. **ids**: The id of the tweet ( *2087*)\\n\\n3. **date**: the date of the tweet (*Sat May 16 23:58:44 UTC 2009*)\\n\\n4. **flag**: The query (*lyx*). If there is no query, then this value is NO_QUERY.\\n\\n5. **user**: the user that tweeted (*robotickilldozr*)\\n\\n6.  **text**: the text of the tweet (*Lyx is cool*)\\n\\n\\n### Acknowledgements\\n\\nThe official link regarding the dataset with resources about how it was generated is [here][1]\\nThe official paper detailing the approach is  [here][2]\\n\\nCitation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. *CS224N Project Report, Stanford, 1(2009), p.12*.\\n\\n\\n### Inspiration\\n\\nTo detect severity from tweets. You [may have a look at this][3].\\n\\n[1]: http://%20http://help.sentiment140.com/for-students/\\n[2]: http://bhttp://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\\n[3]: https://www.linkedin.com/pulse/social-machine-learning-h2o-twitter-python-marios-michailidis\",\"viewCount\":639681,\"downloadCount\":97607,\"scriptCount\":369,\"topicCount\":19,\"owner\":{\"id\":111640,\"imageUrl\":\"https://storage.googleapis.com/kaggle-avatars/thumbnails/111640-fb.jpg\",\"name\":\"\\u039C\\u03B1\\u03C1\\u03B9\\u03BF\\u03C2 \\u039C\\u03B9\\u03C7\\u03B1\\u03B7\\u03BB\\u03B9\\u03B4\\u03B7\\u03C2 KazAnova\",\"profileUrl\":\"/kazanova\",\"slug\":\"kazanova\",\"userTier\":\"GRANDMASTER\"},\"license\":{\"id\":12,\"name\":\"Other (specified in description)\"},\"overview\":\"Sentiment analysis with tweets\",\"collaboratorList\":{\"owner\":{\"userId\":111640,\"profileUrl\":\"/kazanova\",\"thumbnailUrl\":\"https://storage.googleapis.com/kaggle-avatars/thumbnails/111640-fb.jpg\",\"name\":\"\\u039C\\u03B1\\u03C1\\u03B9\\u03BF\\u03C2 \\u039C\\u03B9\\u03C7\\u03B1\\u03B7\\u03BB\\u03B9\\u03B4\\u03B7\\u03C2 KazAnova\",\"slug\":\"kazanova\",\"userTier\":\"GRANDMASTER\",\"type\":\"OWNER\"}},\"coverImageUrl\":\"https://storage.googleapis.com/kaggle-datasets-images/2477/4136/16575d5fdeeba8a6b94021e4bf2748d0/dataset-cover.jpg\",\"data\":{\"totalSize\":84855679,\"isFileset\":true,\"canDownload\":true,\"firestorePath\":\"IzFMXDCGhXX56ZBYHScq/versions/WYvuo7hrdNvvgqIc3YBx\",\"downloadUrl\":\"/datasets/kazanova/kazanova/download?datasetVersionNumber=2\",\"versionId\":4140,\"id\":2477},\"voteCount\":1528,\"medalUrl\":\"/static/images/medals/datasets/goldl@2x.png\",\"downloadUrl\":\"/datasets/kazanova/sentiment140/download?datasetVersionNumber=2\",\"lastUpdateTime\":\"2017-09-13T22:43:19.117Z\",\"datasetVersionNumber\":2,\"creatorDisplayName\":\"\\u039C\\u03B1\\u03C1\\u03B9\\u03BF\\u03C2 \\u039C\\u03B9\\u03C7\\u03B1\\u03B7\\u03BB\\u03B9\\u03B4\\u03B7\\u03C2 KazAnova\",\"cardImageUrl\":\"https://storage.googleapis.com/kaggle-datasets-images/2477/4136/16575d5fdeeba8a6b94021e4bf2748d0/dataset-card.jpg\",\"categories\":{\"tags\":[{\"id\":12116,\"name\":\"internet\",\"fullPath\":\"subject \\u003E science and technology \\u003E internet\",\"listingUrl\":\"/datasets?group=public\\u0026tagids=12116\",\"description\":\"An interconnected network of tubes that connects the entire world together. This tag covers a broad range of tags; anything from cryptocurrency to website analytics.\",\"datasetCount\":11127,\"competitionCount\":18,\"notebookCount\":2082,\"tagUrl\":\"/tags/internet\",\"displayName\":\"Internet\",\"fontAwesomeIcon\":\"globe\"},{\"id\":16489,\"name\":\"online communities\",\"fullPath\":\"subject \\u003E science and technology \\u003E internet \\u003E online communities\",\"listingUrl\":\"/datasets?group=public\\u0026tagids=16489\",\"description\":\"\",\"datasetCount\":7078,\"competitionCount\":3,\"notebookCount\":1235,\"tagUrl\":\"/tags/online-communities\",\"displayName\":\"Online Communities\"},{\"id\":16502,\"name\":\"social networks\",\"fullPath\":\"subject \\u003E science and technology \\u003E internet \\u003E online communities \\u003E social networks\",\"listingUrl\":\"/datasets?group=public\\u0026tagids=16502\",\"description\":\"\",\"datasetCount\":3733,\"notebookCount\":4344,\"tagUrl\":\"/tags/social-networks\",\"displayName\":\"Social Networks\"},{\"id\":11208,\"name\":\"linguistics\",\"fullPath\":\"subject \\u003E people and society \\u003E social science \\u003E linguistics\",\"listingUrl\":\"/datasets?group=public\\u0026tagids=11208\",\"description\":\"The linguistics tag contains datasets and kernels that you can use for text analytics, sentiment analyses, and making clever jokes like this: Let me tell you a little about myself. It\\u0027s a reflexive pronoun that means \\u0022me.\\u0022\",\"datasetCount\":682,\"competitionCount\":9,\"notebookCount\":166,\"tagUrl\":\"/tags/linguistics\",\"displayName\":\"Linguistics\",\"fontAwesomeIcon\":\"language\"},{\"id\":2107,\"name\":\"languages\",\"fullPath\":\"subject \\u003E culture and humanities \\u003E languages\",\"listingUrl\":\"/datasets?group=public\\u0026tagids=2107\",\"description\":\"Language is a method of communication that consists of using words arranged into meaningful patterns. This is a good place to find natural language processing datasets and kernels to study languages and train your chat bots.\",\"datasetCount\":585,\"competitionCount\":4,\"notebookCount\":267,\"tagUrl\":\"/tags/languages\",\"displayName\":\"Languages\"}]},\"datasetExternalUrl\":\"https://www.kaggle.com/datasets/kazanova/sentiment140\",\"forumId\":6554,\"datasetVersionId\":4140,\"canCreateTasks\":true,\"moderationStatus\":\"PRIVATED_MODERATION_STATUS_NO_ABUSE\",\"lastVersionNumber\":2,\"diffType\":\"VERSIONED\"},\"@wf\": \"Datasets.DatasetMaterialDtoWireFormat3\"});performance && performance.mark && performance.mark(\"DatasetMaterialContainer.componentCouldBootstrap\");</script><script nonce=\"GpPhwX4/E337PCNOSKRthg==\" type=\"text/x-mathjax-config\">\n",
       "    MathJax.Hub.Config({\n",
       "    \"HTML-CSS\": {\n",
       "    preferredFont: \"TeX\",\n",
       "    availableFonts: [\"STIX\", \"TeX\"],\n",
       "    linebreaks: {\n",
       "    automatic: true\n",
       "    },\n",
       "    EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)\n",
       "    },\n",
       "    tex2jax: {\n",
       "    inlineMath: [[\"\\\\(\", \"\\\\)\"], [\"\\\\\\\\(\", \"\\\\\\\\)\"]],\n",
       "    displayMath: [[\"$$\", \"$$\"], [\"\\\\[\", \"\\\\]\"]],\n",
       "    processEscapes: true,\n",
       "    ignoreClass: \"tex2jax_ignore|dno\"\n",
       "    },\n",
       "    TeX: {\n",
       "    noUndefined: {\n",
       "    attributes: {\n",
       "    mathcolor: \"red\",\n",
       "    mathbackground: \"#FFEEEE\",\n",
       "    mathsize: \"90%\"\n",
       "    }\n",
       "    }\n",
       "    },\n",
       "    Macros: {\n",
       "    href: \"{}\"\n",
       "    },\n",
       "    skipStartupTypeset: true,\n",
       "    messageStyle: \"none\",\n",
       "    extensions: [\"Safe.js\"],\n",
       "    });\n",
       "</script>\n",
       "<script nonce=\"GpPhwX4/E337PCNOSKRthg==\" type=\"text/javascript\">\n",
       "  window.addEventListener(\"DOMContentLoaded\", () => {\n",
       "    const head = document.getElementsByTagName(\"head\")[0];\n",
       "    const useProdHosts = [\"www.kaggle.com\", \"admin.kaggle.com\"];\n",
       "    const subdomain = useProdHosts.includes(window.location.hostname) ? \"www\" : \"staging\";\n",
       "\n",
       "    const lib = document.createElement(\"script\");\n",
       "    lib.type = \"text/javascript\";\n",
       "    lib.src = `https://${subdomain}.kaggleusercontent.com/static/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML`;\n",
       "    head.appendChild(lib);\n",
       "  });\n",
       "</script>\n",
       "</div>\n",
       "</div></main>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "print(response.status_code)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = []\n",
    "# id = []\n",
    "# text = []\n",
    "# date = []\n",
    "# flag = []\n",
    "# user = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('../Sentiment140.csv')\n",
    "raw_data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling to 20 k the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209760</th>\n",
       "      <td>0</td>\n",
       "      <td>1974058893</td>\n",
       "      <td>Sat May 30 12:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BrookeAmanda</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259240</th>\n",
       "      <td>4</td>\n",
       "      <td>1998068077</td>\n",
       "      <td>Mon Jun 01 17:56:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KarinaKornacka</td>\n",
       "      <td>@graceofrhythm HAHA no i would never do that!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266850</th>\n",
       "      <td>4</td>\n",
       "      <td>1999729993</td>\n",
       "      <td>Mon Jun 01 20:43:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stevegaghagen</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300432</th>\n",
       "      <td>4</td>\n",
       "      <td>2006627206</td>\n",
       "      <td>Tue Jun 02 11:26:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Hecie</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228994</th>\n",
       "      <td>4</td>\n",
       "      <td>1991292674</td>\n",
       "      <td>Mon Jun 01 06:46:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>shanaloren</td>\n",
       "      <td>@STO_MAC nah im not mad at u....luv u too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560781</th>\n",
       "      <td>0</td>\n",
       "      <td>2205268283</td>\n",
       "      <td>Wed Jun 17 04:21:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>neilina</td>\n",
       "      <td>Today, not in a mood to do work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125957</th>\n",
       "      <td>0</td>\n",
       "      <td>1834420698</td>\n",
       "      <td>Mon May 18 02:52:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Dirk_Gently</td>\n",
       "      <td>Thanks everyone for the birthday wishes! I rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879548</th>\n",
       "      <td>4</td>\n",
       "      <td>1685464907</td>\n",
       "      <td>Sun May 03 00:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jwalsh</td>\n",
       "      <td>@MFlanders but Vancouver will still steal your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556789</th>\n",
       "      <td>4</td>\n",
       "      <td>2185445238</td>\n",
       "      <td>Mon Jun 15 17:23:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>hippychick_</td>\n",
       "      <td>@keli_h great pic!  So, I'm not the only one t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>0</td>\n",
       "      <td>1468101383</td>\n",
       "      <td>Mon Apr 06 23:43:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Getatnance</td>\n",
       "      <td>@Eyrro awwwww bummerr...sorry missed it again</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "209760        0  1974058893  Sat May 30 12:21:30 PDT 2009  NO_QUERY   \n",
       "1259240       4  1998068077  Mon Jun 01 17:56:23 PDT 2009  NO_QUERY   \n",
       "1266850       4  1999729993  Mon Jun 01 20:43:12 PDT 2009  NO_QUERY   \n",
       "1300432       4  2006627206  Tue Jun 02 11:26:46 PDT 2009  NO_QUERY   \n",
       "1228994       4  1991292674  Mon Jun 01 06:46:16 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "560781        0  2205268283  Wed Jun 17 04:21:37 PDT 2009  NO_QUERY   \n",
       "125957        0  1834420698  Mon May 18 02:52:04 PDT 2009  NO_QUERY   \n",
       "879548        4  1685464907  Sun May 03 00:30:34 PDT 2009  NO_QUERY   \n",
       "1556789       4  2185445238  Mon Jun 15 17:23:33 PDT 2009  NO_QUERY   \n",
       "1183          0  1468101383  Mon Apr 06 23:43:14 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "209760     BrookeAmanda  Ok it's only been a couple hours since dad has...  \n",
       "1259240  KarinaKornacka  @graceofrhythm HAHA no i would never do that!!...  \n",
       "1266850   stevegaghagen  Law of Attraction Creations: Law of Attraction...  \n",
       "1300432           Hecie             is ordering ticketsssss  EEEE (: &lt;3  \n",
       "1228994      shanaloren         @STO_MAC nah im not mad at u....luv u too   \n",
       "...                 ...                                                ...  \n",
       "560781          neilina                   Today, not in a mood to do work   \n",
       "125957      Dirk_Gently  Thanks everyone for the birthday wishes! I rea...  \n",
       "879548           jwalsh  @MFlanders but Vancouver will still steal your...  \n",
       "1556789     hippychick_  @keli_h great pic!  So, I'm not the only one t...  \n",
       "1183         Getatnance     @Eyrro awwwww bummerr...sorry missed it again   \n",
       "\n",
       "[20000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209760</th>\n",
       "      <td>0</td>\n",
       "      <td>1974058893</td>\n",
       "      <td>Sat May 30 12:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BrookeAmanda</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259240</th>\n",
       "      <td>4</td>\n",
       "      <td>1998068077</td>\n",
       "      <td>Mon Jun 01 17:56:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KarinaKornacka</td>\n",
       "      <td>@graceofrhythm HAHA no i would never do that!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266850</th>\n",
       "      <td>4</td>\n",
       "      <td>1999729993</td>\n",
       "      <td>Mon Jun 01 20:43:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stevegaghagen</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300432</th>\n",
       "      <td>4</td>\n",
       "      <td>2006627206</td>\n",
       "      <td>Tue Jun 02 11:26:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Hecie</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228994</th>\n",
       "      <td>4</td>\n",
       "      <td>1991292674</td>\n",
       "      <td>Mon Jun 01 06:46:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>shanaloren</td>\n",
       "      <td>@STO_MAC nah im not mad at u....luv u too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972728</th>\n",
       "      <td>4</td>\n",
       "      <td>1832655595</td>\n",
       "      <td>Sun May 17 21:12:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>GioWC</td>\n",
       "      <td>@ century room, taste the rainbow with your fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554112</th>\n",
       "      <td>4</td>\n",
       "      <td>2184786636</td>\n",
       "      <td>Mon Jun 15 16:25:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LauraVogel</td>\n",
       "      <td>Ari also got a service award for the community...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636233</th>\n",
       "      <td>0</td>\n",
       "      <td>2233854669</td>\n",
       "      <td>Thu Jun 18 22:00:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Bugzbunny20</td>\n",
       "      <td>man I thought somethin was fina go done she di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042659</th>\n",
       "      <td>4</td>\n",
       "      <td>1957228583</td>\n",
       "      <td>Thu May 28 23:53:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ghbetbeze</td>\n",
       "      <td>@JaneBegger Leaving Moscow when it finally get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123515</th>\n",
       "      <td>0</td>\n",
       "      <td>1833924004</td>\n",
       "      <td>Mon May 18 00:57:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>htmlrulezd00d19</td>\n",
       "      <td>I wish I got a summer break!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "209760        0  1974058893  Sat May 30 12:21:30 PDT 2009  NO_QUERY   \n",
       "1259240       4  1998068077  Mon Jun 01 17:56:23 PDT 2009  NO_QUERY   \n",
       "1266850       4  1999729993  Mon Jun 01 20:43:12 PDT 2009  NO_QUERY   \n",
       "1300432       4  2006627206  Tue Jun 02 11:26:46 PDT 2009  NO_QUERY   \n",
       "1228994       4  1991292674  Mon Jun 01 06:46:16 PDT 2009  NO_QUERY   \n",
       "972728        4  1832655595  Sun May 17 21:12:28 PDT 2009  NO_QUERY   \n",
       "1554112       4  2184786636  Mon Jun 15 16:25:11 PDT 2009  NO_QUERY   \n",
       "636233        0  2233854669  Thu Jun 18 22:00:41 PDT 2009  NO_QUERY   \n",
       "1042659       4  1957228583  Thu May 28 23:53:14 PDT 2009  NO_QUERY   \n",
       "123515        0  1833924004  Mon May 18 00:57:36 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "209760      BrookeAmanda  Ok it's only been a couple hours since dad has...  \n",
       "1259240   KarinaKornacka  @graceofrhythm HAHA no i would never do that!!...  \n",
       "1266850    stevegaghagen  Law of Attraction Creations: Law of Attraction...  \n",
       "1300432            Hecie             is ordering ticketsssss  EEEE (: &lt;3  \n",
       "1228994       shanaloren         @STO_MAC nah im not mad at u....luv u too   \n",
       "972728             GioWC  @ century room, taste the rainbow with your fa...  \n",
       "1554112       LauraVogel  Ari also got a service award for the community...  \n",
       "636233       Bugzbunny20  man I thought somethin was fina go done she di...  \n",
       "1042659        ghbetbeze  @JaneBegger Leaving Moscow when it finally get...  \n",
       "123515   htmlrulezd00d19                      I wish I got a summer break!   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_20k = raw_data.sample(n = 20000, random_state= 88)\n",
    "\n",
    "display(raw_data_20k)\n",
    "print('\\/*\\/*\\/' * 30)\n",
    "\n",
    "raw_data_10 = raw_data.sample(n = 10, random_state= 88)\n",
    "raw_data_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Textual Data for Sentiment Analysis\n",
    "\n",
    "Now, apply the functions you have written in Challenge 1 to your whole data set. These functions include:\n",
    "\n",
    "* `clean_up()`\n",
    "\n",
    "* `tokenize()`\n",
    "\n",
    "* `stem_and_lemmatize()`\n",
    "\n",
    "* `remove_stopwords()`\n",
    "\n",
    "Create a new column called `text_processed` in the dataframe to contain the processed data. At the end, your `text_processed` column should contain lists of word tokens that are cleaned up. Your data should look like below:\n",
    "\n",
    "![Processed Data](data-cleaning-results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def one_go_clean(text):\n",
    "    ps = PorterStemmer()\n",
    "    lem = WordNetLemmatizer() \n",
    "    stop_words = stopwords.words('english')\n",
    "        \n",
    "    pat = re.sub(r'https?://\\S+', '', text) # remove the URL\n",
    "    pat1 = re.sub('\\W+', ' ', pat)  # remove Any non-alphanumeric character \n",
    "    s = re.sub('\\d+', '', pat1).lower()  \n",
    "\n",
    "    l = word_tokenize(s)  # tokenise the words\n",
    "    \n",
    "\n",
    "    stem_text = [ps.stem(w) for w in l]\n",
    "    y = [lem.lemmatize(word) for word in stem_text]\n",
    "\n",
    "\n",
    "    cleanned = [word for word in y if not word in stop_words]\n",
    "\n",
    "    return cleanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 21204.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209760</th>\n",
       "      <td>0</td>\n",
       "      <td>1974058893</td>\n",
       "      <td>Sat May 30 12:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BrookeAmanda</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "      <td>[ok, onli, coupl, hour, sinc, dad, ha, said, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259240</th>\n",
       "      <td>4</td>\n",
       "      <td>1998068077</td>\n",
       "      <td>Mon Jun 01 17:56:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KarinaKornacka</td>\n",
       "      <td>@graceofrhythm HAHA no i would never do that!!...</td>\n",
       "      <td>[graceofrhythm, haha, would, never, actual, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266850</th>\n",
       "      <td>4</td>\n",
       "      <td>1999729993</td>\n",
       "      <td>Mon Jun 01 20:43:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stevegaghagen</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "      <td>[law, attract, creation, law, attract, creatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300432</th>\n",
       "      <td>4</td>\n",
       "      <td>2006627206</td>\n",
       "      <td>Tue Jun 02 11:26:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Hecie</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "      <td>[order, ticket, eeee, lt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228994</th>\n",
       "      <td>4</td>\n",
       "      <td>1991292674</td>\n",
       "      <td>Mon Jun 01 06:46:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>shanaloren</td>\n",
       "      <td>@STO_MAC nah im not mad at u....luv u too</td>\n",
       "      <td>[sto_mac, nah, im, mad, u, luv, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972728</th>\n",
       "      <td>4</td>\n",
       "      <td>1832655595</td>\n",
       "      <td>Sun May 17 21:12:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>GioWC</td>\n",
       "      <td>@ century room, taste the rainbow with your fa...</td>\n",
       "      <td>[centuri, room, tast, rainbow, favorit, photog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554112</th>\n",
       "      <td>4</td>\n",
       "      <td>2184786636</td>\n",
       "      <td>Mon Jun 15 16:25:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LauraVogel</td>\n",
       "      <td>Ari also got a service award for the community...</td>\n",
       "      <td>[ari, also, got, servic, award, commun, servic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636233</th>\n",
       "      <td>0</td>\n",
       "      <td>2233854669</td>\n",
       "      <td>Thu Jun 18 22:00:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Bugzbunny20</td>\n",
       "      <td>man I thought somethin was fina go done she di...</td>\n",
       "      <td>[man, thought, somethin, wa, fina, go, done, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042659</th>\n",
       "      <td>4</td>\n",
       "      <td>1957228583</td>\n",
       "      <td>Thu May 28 23:53:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ghbetbeze</td>\n",
       "      <td>@JaneBegger Leaving Moscow when it finally get...</td>\n",
       "      <td>[janebegg, leav, moscow, final, get, warm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123515</th>\n",
       "      <td>0</td>\n",
       "      <td>1833924004</td>\n",
       "      <td>Mon May 18 00:57:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>htmlrulezd00d19</td>\n",
       "      <td>I wish I got a summer break!</td>\n",
       "      <td>[wish, got, summer, break]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "209760        0  1974058893  Sat May 30 12:21:30 PDT 2009  NO_QUERY   \n",
       "1259240       4  1998068077  Mon Jun 01 17:56:23 PDT 2009  NO_QUERY   \n",
       "1266850       4  1999729993  Mon Jun 01 20:43:12 PDT 2009  NO_QUERY   \n",
       "1300432       4  2006627206  Tue Jun 02 11:26:46 PDT 2009  NO_QUERY   \n",
       "1228994       4  1991292674  Mon Jun 01 06:46:16 PDT 2009  NO_QUERY   \n",
       "972728        4  1832655595  Sun May 17 21:12:28 PDT 2009  NO_QUERY   \n",
       "1554112       4  2184786636  Mon Jun 15 16:25:11 PDT 2009  NO_QUERY   \n",
       "636233        0  2233854669  Thu Jun 18 22:00:41 PDT 2009  NO_QUERY   \n",
       "1042659       4  1957228583  Thu May 28 23:53:14 PDT 2009  NO_QUERY   \n",
       "123515        0  1833924004  Mon May 18 00:57:36 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "209760      BrookeAmanda  Ok it's only been a couple hours since dad has...   \n",
       "1259240   KarinaKornacka  @graceofrhythm HAHA no i would never do that!!...   \n",
       "1266850    stevegaghagen  Law of Attraction Creations: Law of Attraction...   \n",
       "1300432            Hecie             is ordering ticketsssss  EEEE (: &lt;3   \n",
       "1228994       shanaloren         @STO_MAC nah im not mad at u....luv u too    \n",
       "972728             GioWC  @ century room, taste the rainbow with your fa...   \n",
       "1554112       LauraVogel  Ari also got a service award for the community...   \n",
       "636233       Bugzbunny20  man I thought somethin was fina go done she di...   \n",
       "1042659        ghbetbeze  @JaneBegger Leaving Moscow when it finally get...   \n",
       "123515   htmlrulezd00d19                      I wish I got a summer break!    \n",
       "\n",
       "                                            text_processed  \n",
       "209760   [ok, onli, coupl, hour, sinc, dad, ha, said, g...  \n",
       "1259240  [graceofrhythm, haha, would, never, actual, ma...  \n",
       "1266850  [law, attract, creation, law, attract, creatio...  \n",
       "1300432                          [order, ticket, eeee, lt]  \n",
       "1228994                 [sto_mac, nah, im, mad, u, luv, u]  \n",
       "972728   [centuri, room, tast, rainbow, favorit, photog...  \n",
       "1554112  [ari, also, got, servic, award, commun, servic...  \n",
       "636233   [man, thought, somethin, wa, fina, go, done, e...  \n",
       "1042659         [janebegg, leav, moscow, final, get, warm]  \n",
       "123515                          [wish, got, summer, break]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_10['text_processed'] = tqdm(raw_data_10['text'].apply(one_go_clean))\n",
    "raw_data_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Only words in the text_processed:\n",
      "\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\\/*\\/*\\/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_procesed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209760</th>\n",
       "      <td>0</td>\n",
       "      <td>1974058893</td>\n",
       "      <td>Sat May 30 12:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BrookeAmanda</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "      <td>[ok, onli, coupl, hour, sinc, dad, ha, said, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259240</th>\n",
       "      <td>4</td>\n",
       "      <td>1998068077</td>\n",
       "      <td>Mon Jun 01 17:56:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KarinaKornacka</td>\n",
       "      <td>@graceofrhythm HAHA no i would never do that!!...</td>\n",
       "      <td>[graceofrhythm, haha, would, never, actual, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266850</th>\n",
       "      <td>4</td>\n",
       "      <td>1999729993</td>\n",
       "      <td>Mon Jun 01 20:43:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stevegaghagen</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "      <td>[law, attract, creation, law, attract, creatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300432</th>\n",
       "      <td>4</td>\n",
       "      <td>2006627206</td>\n",
       "      <td>Tue Jun 02 11:26:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Hecie</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "      <td>[order, ticket, eeee, lt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228994</th>\n",
       "      <td>4</td>\n",
       "      <td>1991292674</td>\n",
       "      <td>Mon Jun 01 06:46:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>shanaloren</td>\n",
       "      <td>@STO_MAC nah im not mad at u....luv u too</td>\n",
       "      <td>[sto_mac, nah, im, mad, u, luv, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560781</th>\n",
       "      <td>0</td>\n",
       "      <td>2205268283</td>\n",
       "      <td>Wed Jun 17 04:21:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>neilina</td>\n",
       "      <td>Today, not in a mood to do work</td>\n",
       "      <td>[today, mood, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125957</th>\n",
       "      <td>0</td>\n",
       "      <td>1834420698</td>\n",
       "      <td>Mon May 18 02:52:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Dirk_Gently</td>\n",
       "      <td>Thanks everyone for the birthday wishes! I rea...</td>\n",
       "      <td>[thank, everyon, birthday, wish, realli, hope,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879548</th>\n",
       "      <td>4</td>\n",
       "      <td>1685464907</td>\n",
       "      <td>Sun May 03 00:30:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jwalsh</td>\n",
       "      <td>@MFlanders but Vancouver will still steal your...</td>\n",
       "      <td>[mflander, vancouv, still, steal, job, matter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556789</th>\n",
       "      <td>4</td>\n",
       "      <td>2185445238</td>\n",
       "      <td>Mon Jun 15 17:23:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>hippychick_</td>\n",
       "      <td>@keli_h great pic!  So, I'm not the only one t...</td>\n",
       "      <td>[keli_h, great, pic, onli, one, read, girl, tub]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>0</td>\n",
       "      <td>1468101383</td>\n",
       "      <td>Mon Apr 06 23:43:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Getatnance</td>\n",
       "      <td>@Eyrro awwwww bummerr...sorry missed it again</td>\n",
       "      <td>[eyrro, awwwww, bummerr, sorri, miss]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "209760        0  1974058893  Sat May 30 12:21:30 PDT 2009  NO_QUERY   \n",
       "1259240       4  1998068077  Mon Jun 01 17:56:23 PDT 2009  NO_QUERY   \n",
       "1266850       4  1999729993  Mon Jun 01 20:43:12 PDT 2009  NO_QUERY   \n",
       "1300432       4  2006627206  Tue Jun 02 11:26:46 PDT 2009  NO_QUERY   \n",
       "1228994       4  1991292674  Mon Jun 01 06:46:16 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "560781        0  2205268283  Wed Jun 17 04:21:37 PDT 2009  NO_QUERY   \n",
       "125957        0  1834420698  Mon May 18 02:52:04 PDT 2009  NO_QUERY   \n",
       "879548        4  1685464907  Sun May 03 00:30:34 PDT 2009  NO_QUERY   \n",
       "1556789       4  2185445238  Mon Jun 15 17:23:33 PDT 2009  NO_QUERY   \n",
       "1183          0  1468101383  Mon Apr 06 23:43:14 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \\\n",
       "209760     BrookeAmanda  Ok it's only been a couple hours since dad has...   \n",
       "1259240  KarinaKornacka  @graceofrhythm HAHA no i would never do that!!...   \n",
       "1266850   stevegaghagen  Law of Attraction Creations: Law of Attraction...   \n",
       "1300432           Hecie             is ordering ticketsssss  EEEE (: &lt;3   \n",
       "1228994      shanaloren         @STO_MAC nah im not mad at u....luv u too    \n",
       "...                 ...                                                ...   \n",
       "560781          neilina                   Today, not in a mood to do work    \n",
       "125957      Dirk_Gently  Thanks everyone for the birthday wishes! I rea...   \n",
       "879548           jwalsh  @MFlanders but Vancouver will still steal your...   \n",
       "1556789     hippychick_  @keli_h great pic!  So, I'm not the only one t...   \n",
       "1183         Getatnance     @Eyrro awwwww bummerr...sorry missed it again    \n",
       "\n",
       "                                             text_procesed  \n",
       "209760   [ok, onli, coupl, hour, sinc, dad, ha, said, g...  \n",
       "1259240  [graceofrhythm, haha, would, never, actual, ma...  \n",
       "1266850  [law, attract, creation, law, attract, creatio...  \n",
       "1300432                          [order, ticket, eeee, lt]  \n",
       "1228994                 [sto_mac, nah, im, mad, u, luv, u]  \n",
       "...                                                    ...  \n",
       "560781                                 [today, mood, work]  \n",
       "125957   [thank, everyon, birthday, wish, realli, hope,...  \n",
       "879548   [mflander, vancouv, still, steal, job, matter,...  \n",
       "1556789   [keli_h, great, pic, onli, one, read, girl, tub]  \n",
       "1183                 [eyrro, awwwww, bummerr, sorri, miss]  \n",
       "\n",
       "[20000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data_20k['text_procesed'] = raw_data_20k['text'].apply(one_go_clean)\n",
    "print(\"\\n Only words in the text_processed:\")\n",
    "print('\\/*\\/*\\/' * 30)\n",
    "display(raw_data_20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_procesed</th>\n",
       "      <th>clean_blob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209760</th>\n",
       "      <td>0</td>\n",
       "      <td>1974058893</td>\n",
       "      <td>Sat May 30 12:21:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BrookeAmanda</td>\n",
       "      <td>Ok it's only been a couple hours since dad has...</td>\n",
       "      <td>[ok, onli, coupl, hour, sinc, dad, ha, said, g...</td>\n",
       "      <td>ok onli coupl hour sinc dad ha said go alreadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259240</th>\n",
       "      <td>4</td>\n",
       "      <td>1998068077</td>\n",
       "      <td>Mon Jun 01 17:56:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KarinaKornacka</td>\n",
       "      <td>@graceofrhythm HAHA no i would never do that!!...</td>\n",
       "      <td>[graceofrhythm, haha, would, never, actual, ma...</td>\n",
       "      <td>graceofrhythm haha would never actual made sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266850</th>\n",
       "      <td>4</td>\n",
       "      <td>1999729993</td>\n",
       "      <td>Mon Jun 01 20:43:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>stevegaghagen</td>\n",
       "      <td>Law of Attraction Creations: Law of Attraction...</td>\n",
       "      <td>[law, attract, creation, law, attract, creatio...</td>\n",
       "      <td>law attract creation law attract creation today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300432</th>\n",
       "      <td>4</td>\n",
       "      <td>2006627206</td>\n",
       "      <td>Tue Jun 02 11:26:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Hecie</td>\n",
       "      <td>is ordering ticketsssss  EEEE (: &amp;lt;3</td>\n",
       "      <td>[order, ticket, eeee, lt]</td>\n",
       "      <td>order ticket eeee lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228994</th>\n",
       "      <td>4</td>\n",
       "      <td>1991292674</td>\n",
       "      <td>Mon Jun 01 06:46:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>shanaloren</td>\n",
       "      <td>@STO_MAC nah im not mad at u....luv u too</td>\n",
       "      <td>[sto_mac, nah, im, mad, u, luv, u]</td>\n",
       "      <td>sto_mac nah im mad u luv u</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "209760        0  1974058893  Sat May 30 12:21:30 PDT 2009  NO_QUERY   \n",
       "1259240       4  1998068077  Mon Jun 01 17:56:23 PDT 2009  NO_QUERY   \n",
       "1266850       4  1999729993  Mon Jun 01 20:43:12 PDT 2009  NO_QUERY   \n",
       "1300432       4  2006627206  Tue Jun 02 11:26:46 PDT 2009  NO_QUERY   \n",
       "1228994       4  1991292674  Mon Jun 01 06:46:16 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \\\n",
       "209760     BrookeAmanda  Ok it's only been a couple hours since dad has...   \n",
       "1259240  KarinaKornacka  @graceofrhythm HAHA no i would never do that!!...   \n",
       "1266850   stevegaghagen  Law of Attraction Creations: Law of Attraction...   \n",
       "1300432           Hecie             is ordering ticketsssss  EEEE (: &lt;3   \n",
       "1228994      shanaloren         @STO_MAC nah im not mad at u....luv u too    \n",
       "\n",
       "                                             text_procesed  \\\n",
       "209760   [ok, onli, coupl, hour, sinc, dad, ha, said, g...   \n",
       "1259240  [graceofrhythm, haha, would, never, actual, ma...   \n",
       "1266850  [law, attract, creation, law, attract, creatio...   \n",
       "1300432                          [order, ticket, eeee, lt]   \n",
       "1228994                 [sto_mac, nah, im, mad, u, luv, u]   \n",
       "\n",
       "                                                clean_blob  \n",
       "209760   ok onli coupl hour sinc dad ha said go alreadi...  \n",
       "1259240  graceofrhythm haha would never actual made sus...  \n",
       "1266850    law attract creation law attract creation today  \n",
       "1300432                               order ticket eeee lt  \n",
       "1228994                         sto_mac nah im mad u luv u  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def re_blob(row):\n",
    "    return \" \".join(row['text_procesed'])\n",
    "\n",
    "raw_data_20k['clean_blob'] = raw_data_20k.apply(re_blob,axis=1)\n",
    "raw_data_20k.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bag of Words\n",
    "\n",
    "The purpose of this step is to create a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) from the processed data. The bag of words contains all the unique words in your whole text body (a.k.a. *corpus*) with the number of occurrence of each word. It will allow you to understand which words are the most important features across the whole corpus.\n",
    "\n",
    "Also, you can imagine you will have a massive set of words. The less important words (i.e. those of very low number of occurrence) do not contribute much to the sentiment. Therefore, you only need to use the most important words to build your feature set in the next step. In our case, we will use the top 5,000 words with the highest frequency to build the features.\n",
    "\n",
    "In the cell below, combine all the words in `text_processed` and calculate the frequency distribution of all words. A convenient library to calculate the term frequency distribution is NLTK's `FreqDist` class ([documentation](https://www.nltk.org/api/nltk.html#module-nltk.probability)). Then select the top 5,000 words from the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import nltk \n",
    "from nltk import FreqDist\n",
    "\n",
    "# List of list of all words\n",
    "list_of_words = raw_data_20k[\"text_procesed\"].tolist()\n",
    "list_of_words\n",
    "\n",
    "# Create one list of all words\n",
    "all_words = []\n",
    "for lists in list_of_words:\n",
    "    for word in lists:\n",
    "        all_words.append(word)\n",
    "\n",
    "freq = FreqDist(raw_data_20k['clean_blob'])\n",
    "\n",
    "top_5k_words = freq.most_common(5000)  # list of the 5k most frequents words\n",
    "print(len(top_5k_words))\n",
    "# print(top_5k_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# separating list of words and frequency of words\n",
    "word_features, word_freq = [[x for x,y in top_5k_words],\n",
    "      [y for x,y in top_5k_words]]\n",
    "\n",
    "# List of just words\n",
    "print(len(word_features))\n",
    "print(len(word_freq))\n",
    "# print(freq_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Features\n",
    "\n",
    "Now let's build the features. Using the top 5,000 words, create a 2-dimensional matrix to record whether each of those words is contained in each document (tweet). Then you also have an output column to indicate whether the sentiment in each tweet is positive. For example, assuming your bag of words has 5 items (`['one', 'two', 'three', 'four', 'five']`) out of 4 documents (`['A', 'B', 'C', 'D']`), your feature set is essentially:\n",
    "\n",
    "| Doc | one | two | three | four | five | is_positive |\n",
    "|---|---|---|---|---|---|---|\n",
    "| A | True | False | False | True | False | True |\n",
    "| B | False | False | False | True | True | False |\n",
    "| C | False | True | False | False | False | True |\n",
    "| D | True | False | False | False | True | False|\n",
    "\n",
    "However, because the `nltk.NaiveBayesClassifier.train` class we will use in the next step does not work with Pandas dataframe, the structure of your feature set should be converted to the Python list looking like below:\n",
    "\n",
    "```python\n",
    "[\n",
    "\t({\n",
    "\t\t'one': True,\n",
    "\t\t'two': False,\n",
    "\t\t'three': False,\n",
    "\t\t'four': True,\n",
    "\t\t'five': False\n",
    "\t}, True),\n",
    "\t({\n",
    "\t\t'one': False,\n",
    "\t\t'two': False,\n",
    "\t\t'three': False,\n",
    "\t\t'four': True,\n",
    "\t\t'five': True\n",
    "\t}, False),\n",
    "\t({\n",
    "\t\t'one': False,\n",
    "\t\t'two': True,\n",
    "\t\t'three': False,\n",
    "\t\t'four': False,\n",
    "\t\t'five': False\n",
    "\t}, True),\n",
    "\t({\n",
    "\t\t'one': True,\n",
    "\t\t'two': False,\n",
    "\t\t'three': False,\n",
    "\t\t'four': False,\n",
    "\t\t'five': True\n",
    "\t}, False)\n",
    "]\n",
    "```\n",
    "\n",
    "To help you in this step, watch the [following video](https://www.youtube.com/watch?v=-vVskDsHcVc) to learn how to build the feature set with Python and NLTK. The source code in this video can be found [here](https://pythonprogramming.net/words-as-features-nltk-tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Building Features](building-features.jpg)](https://www.youtube.com/watch?v=-vVskDsHcVc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "def find_features(lst, all_words):\n",
    "    word_features = list(all_words)\n",
    "    words = set(lst)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n",
    "\n",
    "word_features\n",
    "features=[]\n",
    "for i,l in enumerate(raw_data_20k[\"text_procesed\"]):\n",
    "    s = [find_features(l,all_words), raw_data_20k[\"target\"].iloc[i]]\n",
    "    z = tuple(s)\n",
    "    features.append(z)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Traininng Naive Bayes Model\n",
    "\n",
    "In this step you will split your feature set into a training and a test set. Then you will create a Bayes classifier instance using `nltk.NaiveBayesClassifier.train` ([example](https://www.nltk.org/book/ch06.html)) to train with the training dataset.\n",
    "\n",
    "After training the model, call `classifier.show_most_informative_features()` to inspect the most important features. The output will look like:\n",
    "\n",
    "```\n",
    "Most Informative Features\n",
    "\t    snow = True            False : True   =     34.3 : 1.0\n",
    "\t  easter = True            False : True   =     26.2 : 1.0\n",
    "\t headach = True            False : True   =     20.9 : 1.0\n",
    "\t    argh = True            False : True   =     17.6 : 1.0\n",
    "\tunfortun = True            False : True   =     16.9 : 1.0\n",
    "\t    jona = True             True : False  =     16.2 : 1.0\n",
    "\t     ach = True            False : True   =     14.9 : 1.0\n",
    "\t     sad = True            False : True   =     13.0 : 1.0\n",
    "\t  parent = True            False : True   =     12.9 : 1.0\n",
    "\t  spring = True            False : True   =     12.7 : 1.0\n",
    "```\n",
    "\n",
    "The [following video](https://www.youtube.com/watch?v=rISOsUaTrO4) will help you complete this step. The source code in this video can be found [here](https://pythonprogramming.net/naive-bayes-classifier-nltk-tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Building and Training NB](nb-model-building.jpg)](https://www.youtube.com/watch?v=rISOsUaTrO4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Naive Bayes Model\n",
    "\n",
    "Now we'll test our classifier with the test dataset. This is done by calling `nltk.classify.accuracy(classifier, test)`.\n",
    "\n",
    "As mentioned in one of the tutorial videos, a Naive Bayes model is considered OK if your accuracy score is over 0.6. If your accuracy score is over 0.7, you've done a great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 1: Improve Model Performance\n",
    "\n",
    "If you are still not exhausted so far and want to dig deeper, try to improve your classifier performance. There are many aspects you can dig into, for example:\n",
    "\n",
    "* Improve stemming and lemmatization. Inspect your bag of words and the most important features. Are there any words you should furuther remove from analysis? You can append these words to further remove to the stop words list.\n",
    "\n",
    "* Remember we only used the top 5,000 features to build model? Try using different numbers of top features. The bottom line is to use as few features as you can without compromising your model performance. The fewer features you select into your model, the faster your model is trained. Then you can use a larger sample size to improve your model accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 2: Machine Learning Pipeline\n",
    "\n",
    "In a new Jupyter Notebook, combine all your codes into a function (or a class). Your new function will execute the complete machine learning pipeline job by receiving the dataset location and output the classifier. This will allow you to use your function to predict the sentiment of any tweet in real time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 3: Apache Spark\n",
    "\n",
    "If you have completed the Apache Spark advanced topic lab, what you can do is to migrate your pipeline from local to a Databricks Notebook. Share your notebook with your instructor and classmates to show off your achievements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b3bd93e6e993b5a0362fec8f7339ef1ba3066e57f882c75a84b879c84a67f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
